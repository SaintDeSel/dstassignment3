---
title: "dst_2_sam"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Before we start, we'll load some useful packages:

```{r}
library(tidyverse)
library(naivebayes)
library(caret)
```

Importing the KDD11 dataset:

```{r}
kddata<-read.csv("//Users//samanthawise//Documents//VersionControl//datasciencetoolbox//assignment_1//data//kddcup.data_10_percent") # edit path

kddnames <- read.table("//Users//samanthawise//Documents//VersionControl//datasciencetoolbox//assignment_1//data//kddcup.names",sep=":",skip=1,as.is=T) # edit path

colnames(kddata) <- c(kddnames[,1],"label")

kddata$label <- as.character(kddata$label)
```

We're going to try to predict the protocol type.

```{r}
table(kddata[,"protocol_type"])
table(kddata[,"duration"]==0)
table(kddata[kddata$duration != 0,]$protocol_type)
table(kddata[kddata$duration == 0,]$protocol_type)
```


So there is no zero duration data for the protocol icmp. However for zero duration there is double the amount of icmp then udp. (should we perform log transform on nonzero duration?)

ICMP is a control protocol, meaning that it designed to not carry application data, but rather information about the status of the network itself. 

Both Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) are transportation protocols, they are used to pass the actual data. The main difference between TCP and UDP is that TCP is a connection oriented protocol, it guarantees that all sent packets will reach the destination in the correct order.

UDP, on the other hand, is a connection-less protocol. Communication is datagram oriented, so the integrity is guaranteed only on the single datagram. Datagrams reach destination and can arrive out of order or don't arrive at all. It's generally used for real time communication, where a little percentage of packet loss rate is preferable to the overhead of a TCP connection.

```{r}
trans <- function(x){
  x[,"logduration"]=log10(x[,"duration"])
  x[,"zeroduration"]=(x[,"duration"]==0)
  x
}
kddata2 <- trans(kddata)

kddata2non <- kddata2 %>%
  subset(zeroduration == FALSE) %>%
  mutate(protocol_type = factor(protocol_type)) %>%
  select(duration, protocol_type, count:dst_host_srv_rerror_rate)
  #select(duration, protocol_type, same_srv_rate, diff_srv_rate, srv_diff_host_rate, dst_host_count, dst_host_srv_count, dst_host_same_srv_rate, dst_host_diff_srv_rate, dst_host_same_src_port_rate)


kddata2non_harding <- kddata2non %>%
  mutate(protocol_type = factor(as.numeric(factor(protocol_type)))) #since lvq works on binarised data

kddata2zero <- kddata2 %>%
  subset(zeroduration == TRUE)
  
```

We'll now make a test dataset and a training dataset. First we consider the nonzero duration scenario and we wish to see if we can classify between UDP and TCP:


```{r}
set.seed(1)
n <- dim(kddata2non_harding)[1]
s <- sample(1:n,n/2)
train <- kddata2non_harding[s,]
test <- kddata2non_harding[-s,]
```

Naive Bayes Model

```{r}
#m <- naive_bayes(protocol_type ~ ., data = train)
#m <- naive_bayes(protocol_type ~ ., data = train, laplace = 1)

x_w <- kddata2non %>%
  select(duration, count:dst_host_srv_rerror_rate)
  #select(duration, same_srv_rate:dst_host_same_src_port_rate)

y_w <- kddata2non$protocol_type

model_w <- train(x,y,'naive_bayes',trControl=trainControl(method='cv',number=10))
```

LVQ model

```{r}
x_h <- kddata2non_harding %>%
  select(duration, count:dst_host_srv_rerror_rate)

y_h <- kddata2non_harding$protocol_type

model_h <- train(x_harding,y,'lvq',trControl=trainControl(method='cv',number=10))

```

Making predictions. How many classification errors were made?

Naive Bayes:

```{r}
#predict_naive <- predict(m, test)
#predict_naive_prob <- predict(m, test, type = "prob")
pred_w <- predict(model_w$finalModel,x_w)
confusionMatrix(pred_w, y_w)
```

LVQ

```{r}
require(class)
pred_h <- lvqtest(model_h$finalModel,x_h)
confusionMatrix(pred_h, y)
```

Plotting the features of Naive Bayes:

```{r}
naive_protocol <- naive_bayes(protocol_type ~ ., data = kddata2non)
plot(naive_protocol)
```

Tried to customise the LVQ predict function but since it is not included in the caret package, the caretList function would fail.

```{r}
# lvq_fun <- getModelInfo("lvq")[[1]]
# lvq_fun$prob <- function (modelFit, newdata, submodels = NULL)  {
#   out <- exp(predict(modelFit, newdata))
#   t(apply(out, 1, function(x) x/sum(x)))
# }
```

Stacking

```{r}
# Example of Stacking algorithms
# create submodels
library(caret)
library(caretEnsemble)
# control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
control <- trainControl(method="cv", number=10, classProbs = TRUE)
algorithmList <- c('naive_bayes', 'adaboost')
set.seed(7)
models <- caretList(protocol_type ~., data=kddata2non, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
```

When we are combining the predictions of different models via stacking, it is desirable that the predictions made by the sub-models have low correlation. This would suggest that the models are skillful but in different ways, allowing a new classifier to figure out how to get the best from each model for an improved score.

```{r}
# correlation between results
modelCor(results)
splom(results)
```

We can see that the pair of predictions have a low correlation.

We combine the predictions of the classifiers using random forest along the dataset. 

```{r}
# stack use the random forest algorithm to combine the predictions.
# stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
stackControl <- trainControl(method="cv", number=10, classProbs = TRUE)
set.seed(7)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
```

We can see that this has lifted the accuracy to 99.255%, which is an improvement on the Naive Bayes Model and adaboost alone.

```{r}
# Generate level-one dataset for training the ensemble metalearner
#predDF <- data.frame(pred_w, pred_w)
#modelStack <- train(protocol_type ~ ., data = predDF, method = "rf")
```

```{r}
#library("pROC")
#library("plotROC")

ctrl <- trainControl(method = "repeatedcv",   # 10fold cross validation
                     number = 5,							# do 5 repititions of cv
                     summaryFunction=twoClassSummary,	# Use AUC to pick the best model
                     classProbs=TRUE,
                     allowParallel = TRUE)
 
sam_w_model <- train(x=train,y=y_w,
                              method = "nb",
                              metric = "ROC",
                              trControl = ctrl,
                              #tuneGrid=grid,
                              verbose=FALSE)

sam_w_pred <- predict(sam_w_model,y_w)

 

pred_w_prob <- predict(model_w$finalModel,x_w, type = "prob")

result_roc <- roc(y_w, result.predicted.prob$versicolor)

# selectedIndices <- model_w$pred$mtry == 2
# plot.roc(model_w$pred$obs[selectedIndices],
#          model_w$pred$M[selectedIndices])

# g <- ggplot(model_w$pred[selectedIndices, ], aes(m=M, d=factor(obs, levels = c("R", "M")))) + 
#   geom_roc(n.cuts=0) + 
#   coord_equal() +
#   style_roc()
# 
# g + annotate("text", x=0.75, y=0.25, label=paste("AUC =", round((calc_auc(g))$AUC, 4)))
```


Comparing this performance with Neural Networks: (Kish)
